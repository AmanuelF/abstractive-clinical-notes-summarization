{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "knowledge_retriever_clinical_notes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS0BC5oErcAP",
        "outputId": "1eb126ae-437f-4d4b-b0e9-646a768c287b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT8qQNCSreSW",
        "outputId": "e805575b-38de-4108-cae2-890fffccd3c6"
      },
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/SCH_Proposal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/SCH_Proposal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5qKrgfQYmVg",
        "outputId": "76ed0395-72b8-470c-b9b1-f86f6bb2a134"
      },
      "source": [
        "!pip3 install -q scispacy\n",
        "!pip3 install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz\n",
        "!pip3 install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz\n",
        "\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 44 kB 2.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 12.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 71 kB 11.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 34.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 188 kB 75.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 58.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 628 kB 62.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 54.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 33.1 MB 126 kB/s \n",
            "\u001b[?25h  Building wheel for en-core-sci-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 125.1 MB 35 kB/s \n",
            "\u001b[?25h  Building wheel for en-ner-bc5cdr-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.13)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.10.8)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eq2dLdx2XSm"
      },
      "source": [
        "!pip3 install -q jsonlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l85klovdbwxW",
        "outputId": "e5d71a3f-8a72-49ed-ce91-09fac8bd94d1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle as pk\n",
        "from pprint import pprint\n",
        "import os\n",
        "\n",
        "import spacy\n",
        "import scispacy\n",
        "from spacy import displacy\n",
        "import en_core_sci_sm\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "from scispacy.linking import EntityLinker\n",
        "from collections import (OrderedDict,Counter, defaultdict)\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from pprint import pprint\n",
        "import json\n",
        "import jsonlines\n",
        "from ast import literal_eval\n",
        "from sklearn import metrics\n",
        "from scipy.spatial import distance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:718: UserWarning: [W094] Model 'en_core_sci_sm' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.7,<3.1.0\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI1Ur2cYRMaY"
      },
      "source": [
        "## https://towardsdatascience.com/understanding-faiss-619bb6db2d1a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVUhg_D8D0XJ"
      },
      "source": [
        "## Now, onto the actual knowledge retriever (from UMLS etc) component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WofvL4Yhh1gD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7b14c8-6b16-4590-f77c-6458efa2d2cf"
      },
      "source": [
        "!pip3 install -q transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4 MB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 51.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 66.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 62.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 61 kB 637 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCr_X840hySY"
      },
      "source": [
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtNRq4Bedx6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6065b877-ccd5-4f17-8724-5cee836edefa"
      },
      "source": [
        "!pip3 install -q biobert-embedding==0.1.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 748.9 MB 699 bytes/s \n",
            "\u001b[K     |████████████████████████████████| 123 kB 70.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 73.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 70.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 72.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 68.0 MB/s \n",
            "\u001b[?25h  Building wheel for biobert-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.2.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.2.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.2.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18FXRE0mhjgz"
      },
      "source": [
        "from biobert_embedding.embedding import BiobertEmbedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsGkLRsWhp5b"
      },
      "source": [
        "biobert = BiobertEmbedding()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install owlready2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWd7rPRnZh0h",
        "outputId": "3e0b1677-bc02-4d3a-cf7d-ef85775e4ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting owlready2\n",
            "  Downloading Owlready2-0.35.tar.gz (23.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.8 MB 1.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: owlready2\n",
            "  Building wheel for owlready2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for owlready2: filename=Owlready2-0.35-cp37-cp37m-linux_x86_64.whl size=20432818 sha256=63d548646bb68cdfcff75a094ff24fd59d5be6aac1245a9d2a5a2dad9368e6d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/85/8f/4f254dc1d3b7901c23004544f7214748607d8b0c2c02b3c868\n",
            "Successfully built owlready2\n",
            "Installing collected packages: owlready2\n",
            "Successfully installed owlready2-0.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNPxzWuIUFgq"
      },
      "source": [
        "from owlready2 import *\n",
        "from owlready2.pymedtermino2 import *\n",
        "from owlready2.pymedtermino2.umls import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDr0BcMwqKzK",
        "outputId": "2cafbd89-aa5f-48d0-ac65-c314e54e69e7"
      },
      "source": [
        "#default_world.set_backend(filename = \"pym.sqlite3\")\n",
        "import_umls(\"../apex-codes/entity_sum/umls-2021AA-full.zip\", terminologies = [\"ICD10\", \"SNOMEDCT_US\", \"CUI\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing UMLS from ../apex-codes/entity_sum/umls-2021AA-full.zip with Python version 3.7.12 and Owlready version 2-0.35...\n",
            "Full UMLS release - importing UMLS from inner Zip file 2021AA-full/2021aa-1-meta.nlm...\n",
            "  Parsing 2021AA/META/MRSTY.RRF.gz as MRSTY with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRRANK.RRF.gz as MRRANK with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRCONSO.RRF.aa.gz as MRCONSO with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRCONSO.RRF.ab.gz as MRCONSO with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRDEF.RRF.gz as MRDEF with encoding UTF-8\n",
            "Full UMLS release - importing UMLS from inner Zip file 2021AA-full/2021aa-2-meta.nlm...\n",
            "  Parsing 2021AA/META/MRREL.RRF.aa.gz as MRREL with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRREL.RRF.ab.gz as MRREL with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRREL.RRF.ac.gz as MRREL with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRREL.RRF.ad.gz as MRREL with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRSAT.RRF.aa.gz as MRSAT with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRSAT.RRF.ab.gz as MRSAT with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRSAT.RRF.ac.gz as MRSAT with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRSAT.RRF.ad.gz as MRSAT with encoding UTF-8\n",
            "  Parsing 2021AA/META/MRSAT.RRF.ae.gz as MRSAT with encoding UTF-8\n",
            "Breaking ORIG cycles...\n",
            "    SNOMEDCT_US : 0 cycles found: \n",
            "    ICD10 : 0 cycles found: \n",
            "    SRC : 0 cycles found: \n",
            "Finalizing only properties and restrictions...\n",
            "Finalizing CUI - ORIG mapping...\n",
            "FTS Indexing...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "get_ontology(\"http://PYM/\")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAzs5WrMUFWE"
      },
      "source": [
        "PYM = get_ontology(\"http://PYM/\").load()\n",
        "default_world.save()\n",
        "CUI = PYM[\"CUI\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm_1yVEHUbee"
      },
      "source": [
        "### Method to retrieve top-K facts from background KB for each sentence having the named entities extracted. First, tokenize the input_text into sentences and for each named entity, grab the top-K facts and measure semantic similarity using FAISS with the sentence in which the named entity appears in. Ablation study to be conducted with different values of K (5, 10, 15)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80U0nC2LUFRa"
      },
      "source": [
        "# the dict_data passed here is a json object with article_text, article_abstrat, article_text_named_entities and\n",
        "# article_abstract_named_entities\n",
        "# K - number of facts per named entity to be summarized (here the facts are retrieved such that they should be semantically similar to the \n",
        "# sentences where the named entity appears in)\n",
        "\n",
        "\n",
        "def _get_entity_based_facts(dict_data, input = \"finding\", top_k=5):\n",
        "  dict_knowledge_facts_final = defaultdict(list)   # to store facts extracted from the background knowledge bases (UMLS, ICD-11, SNOMED-CT)\n",
        "  lst_knowledge_facts_total = list()\n",
        "\n",
        "  lst_named_entities_text = list(set(dict_data[f'{input}_entities'].split(' | ')))\n",
        "  \n",
        "  input_text = dict_data[input]   # the input_text here is finding or the impression which will be tokenized into sentences\n",
        "  \n",
        "  input_text_sentences = sent_tokenize(input_text) # this gives us a list of sentences\n",
        "  for named_entity in lst_named_entities_text:\n",
        "    sentences_w_named_entity = [sent for sent in input_text_sentences if named_entity in sent]  # Get sentences with the named entity\n",
        "  \n",
        "    try:\n",
        "      facts_complete = PYM.search(named_entity)\n",
        "\n",
        "      lst_knowledge_facts = list()\n",
        "      for fact_str in facts_complete:        \n",
        "        fact = str(fact_str).split(' # ')[1].strip()   # the actual description linking the named entity pair (disregarding the CUI identifier)\n",
        "        lst_knowledge_facts.append(fact)\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "    sentence_w_named_entities_embeddings = [biobert.sentence_vector(sent).numpy() for sent in sentences_w_named_entity]\n",
        "    # avg embedding of all sentences where the named entity appears in\n",
        "    avg_sentence_w_entities_embedding = np.mean(np.array(sentence_w_named_entities_embeddings), axis=0)   # to be used as a query\n",
        "\n",
        "    # grab top-k facts with high cosine similarity with the avg_sentence_embedding of all sentencew where the named entity appears in\n",
        "    dict_fact_cosine = {}\n",
        "    for fact in lst_knowledge_facts:\n",
        "      fact_biobert_embedding = biobert.sentence_vector(fact)\n",
        "      cosine_sim = 1 - distance.cosine(fact_biobert_embedding, avg_sentence_w_entities_embedding)   # cosine distance between fact embedding and avg sent\n",
        "      dict_fact_cosine[fact] = cosine_sim\n",
        "\n",
        "    # sort the facts based on cosine_sim and return the top-k facts\n",
        "    top_k_facts = list({k: v for k, v in sorted(dict_fact_cosine.items(), \n",
        "                                                key=lambda item: item[1], reverse=True)}.keys())[:top_k]\n",
        "\n",
        "    lst_knowledge_facts_total += top_k_facts   # This is for the entire input_text\n",
        "\n",
        "  knowledge_facts_total_str = \" | \".join(lst_knowledge_facts_total)  # Join all the facts with the pipe separator\n",
        "\n",
        "  return knowledge_facts_total_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main method for the Heart Failure data from UIC"
      ],
      "metadata": {
        "id": "1bmBWkf2eys5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkd3AnDCUNkd"
      },
      "source": [
        "def main():\n",
        "  input_fileName = \"heart_failure_procedures_w_named_entites.jsonl\"\n",
        "  output_fileName = \"heart_failure_procedures_w_named_entites_w_facts.jsonl\"\n",
        "\n",
        "  with open(f\"{input_fileName}\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "      if idx > 2851:\n",
        "        try: \n",
        "          if idx % 1000 == 0 and idx != 0:\n",
        "            print(\"Iteration: \", idx)\n",
        "          dict_data = json.loads(line)\n",
        "\n",
        "          finding_facts = _get_entity_based_facts(dict_data, input = \"finding\")   # call to the knowledge (fact) retriever method--- here it is for source articlce\n",
        "          impression_facts = _get_entity_based_facts(dict_data, input = \"impression\")   # call to the knowledge (fact) retriever method--- here it is for source articlce\n",
        "\n",
        "          dict_1 = {\"record_id\" : dict_data['record_id'],\n",
        "                    \"procedure_name\" : dict_data['procedure_name'],\n",
        "                    \"finding\" : dict_data['finding'],\n",
        "                    \"impression\" : dict_data['impression'],\n",
        "                    \"finding_entities\" : dict_data['finding_entities'],\n",
        "                    \"impression_entities\" : dict_data['impression_entities'],\n",
        "                    \"finding_facts\" : finding_facts,\n",
        "                    \"impression_facts\" : impression_facts\n",
        "                    }\n",
        "              \n",
        "          with jsonlines.open(output_fileName, \"a\") as writer:\n",
        "            writer.write(dict_1)\n",
        "          writer.close()\n",
        "        except:\n",
        "          continue\n",
        "\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiPiesb7UNgL"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Method for the Indiana University benchmark dataset"
      ],
      "metadata": {
        "id": "zMP5J8PQe2t9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCVOMeKDUWzn"
      },
      "source": [
        "def main():\n",
        "  input_fileName = \"NLMCXR_reports_ecgen_radiology_w_named_entites.jsonl\"\n",
        "  output_fileName = \"NLMCXR_reports_ecgen_radiology_w_named_entites_w_facts.jsonl\"\n",
        "\n",
        "  with open(f\"{input_fileName}\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "      try: \n",
        "        if idx % 1663 == 0 and idx != 0:\n",
        "          print(\"Iteration: \", idx)\n",
        "        dict_data = json.loads(line)\n",
        "\n",
        "        finding_facts = _get_entity_based_facts(dict_data, input = \"finding\")   # call to the knowledge (fact) retriever method--- here it is for source articlce\n",
        "        impression_facts = _get_entity_based_facts(dict_data, input = \"impression\")   # call to the knowledge (fact) retriever method--- here it is for source articlce\n",
        "\n",
        "        dict_1 = {\"indication\" : dict_data['indication'],\n",
        "                  \"finding\" : dict_data['finding'],\n",
        "                  \"impression\" : dict_data['impression'],\n",
        "                  \"finding_entities\" : dict_data['finding_entities'],\n",
        "                  \"impression_entities\" : dict_data['impression_entities'],\n",
        "                  \"finding_facts\" : finding_facts,\n",
        "                  \"impression_facts\" : impression_facts\n",
        "                  }\n",
        "            \n",
        "        with jsonlines.open(output_fileName, \"a\") as writer:\n",
        "          writer.write(dict_1)\n",
        "        writer.close()\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5vqgk0tUWnX"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OgbSa1pUWiO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}