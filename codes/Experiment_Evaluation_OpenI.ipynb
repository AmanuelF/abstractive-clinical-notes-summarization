{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment_Evaluation_OpenI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS0BC5oErcAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6c8805-6791-49bf-e4de-3f621d31c580"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT8qQNCSreSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2113639d-5942-48c2-a208-a44a7b578a5a"
      },
      "source": [
        "%cd drive/My\\ Drive/Colab\\ Notebooks/SCH_Proposal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/SCH_Proposal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOdoqhT7IJB-"
      },
      "source": [
        "## The ROUGE evaluation is done between a generated summary for a cited article and its abstract (both can be accessed using their cited ids from the respective folders)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiH2PcFKNyOd"
      },
      "source": [
        "!pip3 install -q rouge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3VOaVprJMUu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from rouge import Rouge\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHy5PRF7IC_H"
      },
      "source": [
        "## Evaluation against the ground truth summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RQXW6EJ25D"
      },
      "source": [
        "def _compute_ROUGE(generated_summary, human_summary):  \n",
        "  rouge = Rouge()\n",
        "  \n",
        "  scores = rouge.get_scores(generated_summary, human_summary)[0]\n",
        "  \n",
        "  rouge_1_f = scores['rouge-1']['f']\n",
        "  rouge_2_f = scores['rouge-2']['f']\n",
        "  rouge_l_f = scores['rouge-l']['f']\n",
        "\n",
        "  rouge_1_f = rouge_1_f * 100\n",
        "  rouge_2_f = rouge_2_f * 100\n",
        "  rouge_l_f = rouge_l_f * 100\n",
        "\n",
        "  return rouge_1_f, rouge_2_f, rouge_l_f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb2Rr87XwELf"
      },
      "source": [
        "## Iterate through the directories housing the generated and human summaries and store into a container and do evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIEgj4o_NEo0",
        "outputId": "50ecd0bd-79eb-4b04-e0a3-16231993a611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/SCH_Proposal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq71hbOVqSul"
      },
      "source": [
        "### Call to the ROUGE evaluation method in main method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrICaBCgy0I6"
      },
      "source": [
        "def main():\n",
        "  lst_modelName = [\"T5\"]\n",
        "  lst_model_type = [\"vanilla\", \"w_named_entities\"]\n",
        "  #lst_model_type = [\"vanilla\", \"w_named_entity\"]\n",
        "  ROOT_SUMMARY_PATH = \"NLMCXR_reports_ecgen_radiology-FINAL-SUMMARIES\"\n",
        "  inference_type = \"vanilla_at_inference\"\n",
        "  #inference_type = \"w_named_entities_at_inference\"\n",
        "  output_file_name = f\"NLMCXR_reports_ecgen_radiology_rouge_scores_wrt_input_article_{inference_type}.xlsx\"\n",
        "  evaluation_wrt_ground_truth = True    # changes based on what target to evaluate the generated to\n",
        "\n",
        "  dict_rouge_scores = defaultdict(list)\n",
        "\n",
        "  for modelName in lst_modelName:\n",
        "    for model_type in lst_model_type:\n",
        "      SUMMARY_PATH = f\"{ROOT_SUMMARY_PATH}/{modelName}_{inference_type}\"   # path changes with the type of input at inference time (w or wo named entities)\n",
        "      #SUMMARY_PATH = f\"pubmed-FINAL-SUMMARIES_w_named_entites_at_inference/{modelName}\"   # path of summaries generated with named entities at inference\n",
        "      input_filename_path = f\"{SUMMARY_PATH}/NLMCXR_reports_ecgen_radiology-summaries-{model_type}.jsonl\"\n",
        "\n",
        "      output_results_path = f\"FINAL_RESULTS/ROUGE\"\n",
        "      os.makedirs(output_results_path, exist_ok=True)\n",
        "      \n",
        "\n",
        "      rouge_1_f_sum, rouge_2_f_sum, rouge_l_f_sum = 0.0, 0.0, 0.0   # initialize all cumulative scores to zero\n",
        "      total_no_summaries = 0\n",
        "      with open(input_filename_path) as fp:\n",
        "        for iter, line in enumerate(fp):\n",
        "          if iter % 1000 == 0 and iter != 0:\n",
        "            print(\"Iteration: \", iter)\n",
        "          dict_data = json.loads(line)\n",
        "\n",
        "          generated_summary = dict_data[\"abstractive_summary\"]    # generated summary\n",
        "\n",
        "          if evaluation_wrt_ground_truth:\n",
        "            evaluation_target = dict_data[\"impression\"]   # human-like summary as evaluation target\n",
        "          else:\n",
        "            evaluation_target = dict_data[\"finding\"]   # source input article----to do evaluation wrt the input article\n",
        "\n",
        "          # call to the ROUGE computing method\n",
        "          try:\n",
        "            rouge_1_f, rouge_2_f, rouge_l_f = _compute_ROUGE(generated_summary, evaluation_target)\n",
        "\n",
        "            rouge_1_f_sum += rouge_1_f\n",
        "            rouge_2_f_sum += rouge_2_f\n",
        "            rouge_l_f_sum += rouge_l_f\n",
        "            \n",
        "            total_no_summaries += 1\n",
        "\n",
        "          except:\n",
        "            continue\n",
        "\n",
        "      fp.close()\n",
        "\n",
        "      avg_rouge_1_f = rouge_1_f_sum/float(total_no_summaries)\n",
        "      avg_rouge_2_f = rouge_2_f_sum/float(total_no_summaries)\n",
        "      avg_rouge_l_f = rouge_l_f_sum/float(total_no_summaries)\n",
        "\n",
        "      dict_rouge_scores[\"training-config\"].append(f\"{modelName}-{model_type}\")\n",
        "      dict_rouge_scores[\"ROUGE-1\"].append(avg_rouge_1_f)\n",
        "      dict_rouge_scores[\"ROUGE-2\"].append(avg_rouge_2_f)\n",
        "      dict_rouge_scores[\"ROUGE-L\"].append(avg_rouge_l_f)\n",
        "\n",
        "      pprint(dict_rouge_scores)\n",
        "\n",
        "  df_rouge_scores = pd.DataFrame(dict_rouge_scores)\n",
        "\n",
        "  print(df_rouge_scores)\n",
        "\n",
        "  df_rouge_scores.to_excel(f\"{output_results_path}/{output_file_name}\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzurVo2Dy0DU",
        "outputId": "fae31387-fe71-436d-c5de-ad00e1d5eb33"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  1000\n",
            "Iteration:  2000\n",
            "Iteration:  3000\n",
            "defaultdict(<class 'list'>,\n",
            "            {'ROUGE-1': [35.11325282585441],\n",
            "             'ROUGE-2': [21.502578763151647],\n",
            "             'ROUGE-L': [34.9207942666428],\n",
            "             'training-config': ['T5-vanilla']})\n",
            "Iteration:  1000\n",
            "Iteration:  2000\n",
            "Iteration:  3000\n",
            "Iteration:  4000\n",
            "Iteration:  5000\n",
            "Iteration:  6000\n",
            "Iteration:  7000\n",
            "Iteration:  8000\n",
            "Iteration:  9000\n",
            "Iteration:  10000\n",
            "defaultdict(<class 'list'>,\n",
            "            {'ROUGE-1': [35.11325282585441, 10.956973232592574],\n",
            "             'ROUGE-2': [21.502578763151647, 2.777513538689189],\n",
            "             'ROUGE-L': [34.9207942666428, 10.780022823065202],\n",
            "             'training-config': ['T5-vanilla', 'T5-w_named_entities']})\n",
            "       training-config    ROUGE-1    ROUGE-2    ROUGE-L\n",
            "0           T5-vanilla  35.113253  21.502579  34.920794\n",
            "1  T5-w_named_entities  10.956973   2.777514  10.780023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_B6L80x5ATs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjgPm1ILOJ1b"
      },
      "source": [
        "## Entity-level Factual consistency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjwuzzlqPz6x",
        "outputId": "0bd4aa2c-80d8-4e6c-e27f-bc7937b6c5b2"
      },
      "source": [
        "!pip3 install -q scispacy\n",
        "!pip3 install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz\n",
        "!pip3 install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz\n",
        "\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 44 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 12.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 2.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 71 kB 9.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 188 kB 68.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 43.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 628 kB 61.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 73.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 33.1 MB 85 kB/s \n",
            "\u001b[?25h  Building wheel for en-core-sci-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 125.1 MB 33 kB/s \n",
            "\u001b[?25h  Building wheel for en-ner-bc5cdr-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.62.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.13)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.2)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.8.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTiVBvHPQNuH"
      },
      "source": [
        "!pip3 install -q jsonlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysFfq4Y-P4d1",
        "outputId": "efa00892-3102-49fb-bf05-20eb801bfe35"
      },
      "source": [
        "import spacy\n",
        "import scispacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle as pk\n",
        "from pprint import pprint\n",
        "import os\n",
        "\n",
        "from spacy import displacy\n",
        "import en_core_sci_sm\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "from scispacy.linking import EntityLinker\n",
        "from collections import (OrderedDict,Counter, defaultdict)\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from pprint import pprint\n",
        "import json\n",
        "import jsonlines\n",
        "from ast import literal_eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/util.py:718: UserWarning: [W094] Model 'en_core_sci_sm' (0.3.0) specifies an under-constrained spaCy version requirement: >=2.3.1. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.0.7,<3.1.0\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYOYWIOV5Ar7"
      },
      "source": [
        "def _get_named_entities(input_text):\n",
        "  nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
        "  doc = nlp(input_text)\n",
        "  entities = []\n",
        "  for ent in doc.ents:\n",
        "    entities.append(ent.text)\n",
        "  str_entities = \" | \".join(entities)   # a string representation of list of entities with the pipe symbol as a separator\n",
        "  \n",
        "  return str_entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osbT2IsLQ60z"
      },
      "source": [
        "### Extract named entities from the generated summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL3u9Y7GPvTa"
      },
      "source": [
        "def main():\n",
        "  lst_modelName = [\"T5\"]\n",
        "  #lst_model_type = [\"vanilla\"]\n",
        "  lst_model_type = [\"vanilla\", \"w_named_entities\"]\n",
        "  ROOT_SUMMARY_PATH = \"NLMCXR_reports_ecgen_radiology-FINAL-SUMMARIES\"\n",
        "  inference_type = \"vanilla_at_inference\"\n",
        "  #inference_type = \"w_named_entities_at_inference\"\n",
        "  output_file_name = f\"NLMCXR_reports_ecgen_radiology_rouge_scores_wrt_input_article_{inference_type}.xlsx\"\n",
        "  evaluation_wrt_ground_truth = True    # changes based on what target to evaluate the generated to\n",
        "\n",
        "  for modelName in lst_modelName:\n",
        "    for model_type in lst_model_type:\n",
        "      SUMMARY_PATH = f\"{ROOT_SUMMARY_PATH}/{modelName}_{inference_type}\"\n",
        "      input_filename_path = f\"{SUMMARY_PATH}/NLMCXR_reports_ecgen_radiology-summaries-{model_type}.jsonl\"\n",
        "\n",
        "      OUTPUT_RESULTS_PATH = f\"FINAL_RESULTS/factual_consistency\"\n",
        "      os.makedirs(OUTPUT_RESULTS_PATH, exist_ok=True)\n",
        "      \n",
        "      with open(input_filename_path) as fp:\n",
        "        for iter, line in enumerate(fp):\n",
        "          if iter % 2000 == 0:\n",
        "            print(\"Iteration: \", iter)\n",
        "          dict_data = json.loads(line)\n",
        "\n",
        "          summary = dict_data[\"abstractive_summary\"]\n",
        "          summary_named_entities = _get_named_entities(summary)\n",
        "\n",
        "          dict_1 = {\"article_text\" : dict_data['article_text'],\n",
        "                    \"article_abstract\" : dict_data['article_abstract'],\n",
        "                    \"article_text_named_entities\" : dict_data['article_text_named_entities'],\n",
        "                    \"article_abstract_named_entities\" : dict_data['article_abstract_named_entities'],\n",
        "                    \"abstractive_summary\" : dict_data[\"abstractive_summary\"],\n",
        "                    \"abstractive_summary_named_entities\" : summary_named_entities\n",
        "                    }\n",
        "              \n",
        "          with jsonlines.open(f\"{OUTPUT_RESULTS_PATH}/{output_file_name}\", \"a\") as writer:\n",
        "            writer.write(dict_1)\n",
        "          writer.close()\n",
        "\n",
        "      fp.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcvlt00mTYkL"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entity-level Factual Consistency using F1 scores"
      ],
      "metadata": {
        "id": "MyKtKruickNw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TU8dElXTh2Q"
      },
      "source": [
        "def main():\n",
        "  lst_modelName = [\"BART\"]\n",
        "  #lst_model_type = [\"vanilla\"]\n",
        "  lst_model_type = [\"vanilla\", \"w_named_entities\"]\n",
        "  ROOT_SUMMARY_PATH = \"NLMCXR_reports_ecgen_radiology-FINAL-SUMMARIES\"\n",
        "  inference_type = \"vanilla_at_inference\"\n",
        "  #inference_type = \"w_named_entities_at_inference\"\n",
        "  output_file_name = f\"NLMCXR_reports_ecgen_radiology_rouge_scores_wrt_input_article_{inference_type}.xlsx\"\n",
        "  evaluation_wrt_ground_truth = True    # changes based on what target to evaluate the generated to\n",
        "\n",
        "  dict_entity_specificity = defaultdict(list)\n",
        "\n",
        "  output_results_path = f\"FINAL_RESULTS/ENTITY_SPECIFITY\"\n",
        "  os.makedirs(output_results_path, exist_ok=True)\n",
        "  if evaluation_wrt_ground_truth:\n",
        "    output_file_name = \"OpenI_entity_specifity_scores_wrt_ground_truth_w_entity_at_inference.xlsx\"\n",
        "  else:\n",
        "    output_file_name = \"OpenI_entity_specifity_scores_wrt_source_article_w_entity_at_inference.xlsx\"\n",
        "\n",
        "  for modelName in lst_modelName:\n",
        "    for model_type in lst_model_type:\n",
        "      #SUMMARY_PATH = f\"pubmed-FINAL-SUMMARIES/{modelName}\"   # for vanilla input evaluation\n",
        "      SUMMARY_PATH = f\"{ROOT_SUMMARY_PATH}/{modelName}_{inference_type}\"    # for input doc + named entity at inference time\n",
        "      \n",
        "      input_filename_path = f\"{SUMMARY_PATH}/NLMCXR_reports_ecgen_radiology-summaries-{model_type}.jsonl\"\n",
        "\n",
        "      precision_total, recall_total = 0.0, 0.0   # precision and recall for entity specificity\n",
        "      total_no_summaries = 0\n",
        "      with open(input_filename_path) as fp:\n",
        "        for iter, line in enumerate(fp):\n",
        "          if iter % 2000 == 0:\n",
        "            print(\"Iteration: \", iter)\n",
        "          dict_data = json.loads(line)\n",
        "\n",
        "          if evaluation_wrt_ground_truth:\n",
        "            target_named_entities = dict_data['impression_entities'].split(' | ')    # ground truth named entities\n",
        "          else:\n",
        "            target_named_entities = dict_data['finding_entities'].split(' | ')   # source article named entities\n",
        "          abstractive_summary_entities = dict_data['abstractive_summary_entities'].split(' | ')  # named entities in generated summary\n",
        "\n",
        "          common_named_entities = list(set(target_named_entities) & set(abstractive_summary_entities))\n",
        "          \n",
        "          try:\n",
        "            precision = len(common_named_entities) / float(len(abstractive_summary_entities))\n",
        "            recall = len(common_named_entities) / float(len(target_named_entities))\n",
        "\n",
        "            precision_total += precision\n",
        "            recall_total += recall\n",
        "            total_no_summaries += 1\n",
        "          except:\n",
        "            continue\n",
        "\n",
        "      avg_precision = precision_total / float(total_no_summaries) * 100\n",
        "      avg_recall = recall_total / float(total_no_summaries) * 100\n",
        "      f1_score = (2 * avg_precision * avg_recall) / (avg_precision + avg_recall)\n",
        "\n",
        "      dict_entity_specificity[\"training-config\"].append(f\"{modelName}-{model_type}\")\n",
        "      dict_entity_specificity[\"avg_precision\"].append(avg_precision)\n",
        "      dict_entity_specificity[\"avg_recall\"].append(avg_recall)\n",
        "      dict_entity_specificity[\"f1_score\"].append(f1_score)\n",
        "      \n",
        "  df_entity_specifity = pd.DataFrame(dict_entity_specificity)\n",
        "\n",
        "  print(df_entity_specifity)\n",
        "\n",
        "  df_entity_specifity.to_excel(f\"{output_results_path}/{output_file_name}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVoImMMjdw3G",
        "outputId": "b8f592a9-5677-4789-8f9c-0b2cf420efbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0\n",
            "Iteration:  2000\n",
            "Iteration:  0\n",
            "Iteration:  2000\n",
            "         training-config  avg_precision  avg_recall  f1_score\n",
            "0           BART-vanilla       5.777032   29.678843  9.671494\n",
            "1  BART-w_named_entities       0.754594    3.258058  1.225379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAUVE Evaluation"
      ],
      "metadata": {
        "id": "gqGO-ouA7NDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q mauve-text"
      ],
      "metadata": {
        "id": "1x2dQE6GdvGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b172dff-3e31-4f59-9860-c0727765cd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 8.5 MB 6.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q torch>=1.10.0\n",
        "!pip3 install -q git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRL6FgozGxj7",
        "outputId": "c124035d-797f-40e6-bd7c-ed9b1fba816d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 58.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 75.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 61 kB 585 kB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import mauve"
      ],
      "metadata": {
        "id": "sOp2i-APGzv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  lst_modelName = [\"T5\", \"BART\", \"Pegasus\"]\n",
        "  lst_model_type = [\"vanilla\", \"w_named_entities\"]\n",
        "\n",
        "  dict_entity_specificity = defaultdict(list)\n",
        "\n",
        "  for modelName in lst_modelName:\n",
        "    for model_type in lst_model_type:\n",
        "      SUMMARY_PATH = f\"pubmed-FINAL-SUMMARIES/{modelName}\"\n",
        "      input_filename_path = f\"pubmed_FINAL-SUMMARIES-w-named_entities/{modelName}/pubmed-summaries-{model_type}.jsonl\"\n",
        "\n",
        "      lst_generated_summary = []\n",
        "      lst_ground_truth_summary = []\n",
        "\n",
        "      with open(input_filename_path) as fp:\n",
        "        for iter, line in enumerate(fp):\n",
        "          if iter % 2000 == 0:\n",
        "            print(\"Iteration: \", iter)\n",
        "          dict_data = json.loads(line)\n",
        "\n",
        "          lst_generated_summary.append(dict_data[\"abstractive_summary\"])    # generated summary\n",
        "          lst_ground_truth_summary.append(dict_data[\"article_abstract\"])   # human-like summary\n",
        "\n",
        "      fp.close()\n",
        "      \n",
        "      lst_generated_summary = lst_generated_summary[:100]\n",
        "      lst_ground_truth_summary = lst_ground_truth_summary[:100]\n",
        "\n",
        "      out = mauve.compute_mauve(p_text=lst_ground_truth_summary, q_text=lst_generated_summary, verbose=False)\n",
        "\n",
        "      mauve_score = out.mauve  \n",
        "            \n",
        "      dict_entity_specificity[\"training-config\"].append(f\"{modelName}-{model_type}\")\n",
        "      dict_entity_specificity[\"avg_mauve\"].append(mauve_score)\n",
        "      \n",
        "      \n",
        "  df_mauve_scores = pd.DataFrame(dict_entity_specificity)\n",
        "\n",
        "  print(df_mauve_scores)\n",
        "\n",
        "  df_mauve_scores.to_excel(\"pubmed_MAUVE_scores.xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PU3RB-5tG4nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "4cmAio7BG98N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZWPnCVaXHAXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}